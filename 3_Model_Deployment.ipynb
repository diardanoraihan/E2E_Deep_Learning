{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "__What we have done so far__:\n",
    "- We have had the best deep learning model in h5 file format trained using TensorFlow. \n",
    "- Typically, we will not put this procedure in production. TensorFlow based on Python is too large and not optimized for serving/prediction. However, it's optimized for training. Hence, we need to use something else for production.\n",
    "- Instead, we can use TensorFlow Serving, TensorFlow Lite, AWS Lambda, etc. In this case, we have used TensorFlow Lite to convert our model in h5 file format to tflite. This decrease the model size drastically while still maintaining the performance to make the prediction.\n",
    "- In this notebook, we will deploy the model using Docker, AWS Lambda, and AWS API Gateway.\n",
    "\n",
    "__AWS Lambda__:\n",
    "\n",
    "By using AWS Lambda, we can run code without thinking about the servers, that is, __serverless__. Hence, we don't need to rent an instance, instead just define a function and specify what is the input and output. We only need to pay for the time this function is actually run. For example, if our function only needs 2 seconds to run, then we only need to pay for 2 seconds. \n",
    "\n",
    "The good news is that AWS Lambda supports __Docker__. Thus, before we deploy the model on AWS, we can run the model locally on our machine. \n",
    "Here are the steps you can follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Function\n",
    "\n",
    "First, we need to create a `lambda_function.py` to deploy the model either on AWS Lambda or Docker since both options need this file for a deep learning model to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in AWS Lambda, we need to use this import below\n",
    "# import tflite_runtime.interpreter as tflite\n",
    "import tensorflow.lite as tflite\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# Create an interpreter interface for any model in TFLite\n",
    "interpreter = tflite.Interpreter(model_path='clothing_classifier.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get a list of input details from the model\n",
    "input_details = interpreter.get_input_details()\n",
    "input_index = input_details[0]['index']\n",
    "\n",
    "# Get a list of output details from the model\n",
    "output_details = interpreter.get_output_details()\n",
    "output_index = output_details[0]['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func: `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    # set the value of the input tensor\n",
    "    interpreter.set_tensor(input_index, X)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the value of the output tensor\n",
    "    preds = interpreter.get_tensor(output_index)\n",
    "    \n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func: `decode_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'short',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "def decode_predictions(pred):\n",
    "    result = {label: float(score) for label, score in zip(labels, pred)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func: `preprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(img_url):\n",
    "    # load the image using PIL module\n",
    "    img = Image.open(urlopen(img_url))\n",
    "    \n",
    "    # Specify the image target size\n",
    "    img = img.resize((150, 150))\n",
    "    \n",
    "    # Turn the image into a 4D-array\n",
    "    X = np.expand_dims(img, axis =0)\n",
    "    \n",
    "    # Normalize the image\n",
    "    X = X/255.0\n",
    "    \n",
    "    # Turn the image into a Numpy array with float32 data type\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func: `lambda_handler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    # Obtain the image location\n",
    "    url = event['url']\n",
    "    \n",
    "    # Preprocess the image\n",
    "    X = preprocessor(url)\n",
    "    \n",
    "    # Make prediction\n",
    "    preds = predict(X)\n",
    "    \n",
    "    # Obtain the result\n",
    "    results = decode_predictions(preds)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': 4.461161218216603e-09,\n",
       " 'hat': 2.6467307959556798e-14,\n",
       " 'longsleeve': 0.02351023070514202,\n",
       " 'outwear': 3.530597038683969e-13,\n",
       " 'pants': 9.088156659523006e-13,\n",
       " 'shirt': 3.83664740866152e-07,\n",
       " 'shoes': 1.397734902410014e-19,\n",
       " 'short': 1.0869638522592595e-11,\n",
       " 'skirt': 2.561472355836619e-13,\n",
       " 't-shirt': 0.9764893651008606}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate the event (trigger)\n",
    "event = {'url': 'https://tinyurl.com/clothes-t-shirt'} \n",
    "\n",
    "# Call the lambda_handler\n",
    "results = lambda_handler(event, context=None)\n",
    "\n",
    "# See the prediction result\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Everything Together: `lambda_function.py`\n",
    "\n",
    "Finally, let's create a file that stores all the functions needed to run the app, starting from defining the interpreter, receiving the input image, preprocessing the image, and use the saved model to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in AWS Lambda, we need to use this import below\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# Create an interpreter interface for any model in TFLite\n",
    "interpreter = tflite.Interpreter(model_path='clothing_classifier.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get a list of input details from the model\n",
    "input_details = interpreter.get_input_details()\n",
    "input_index = input_details[0]['index']\n",
    "\n",
    "# Get a list of output details from the model\n",
    "output_details = interpreter.get_output_details()\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "def predict(X):\n",
    "    # set the value of the input tensor\n",
    "    interpreter.set_tensor(input_index, X)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the value of the output tensor\n",
    "    preds = interpreter.get_tensor(output_index)\n",
    "    \n",
    "    return preds[0]\n",
    "\n",
    "labels = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'short',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "def decode_predictions(pred):\n",
    "    result = {label: float(score) for label, score in zip(labels, pred)}\n",
    "    return result\n",
    "\n",
    "def preprocessor(img_url):\n",
    "    # load the image using PIL module\n",
    "    img = Image.open(urlopen(img_url))\n",
    "    \n",
    "    # Specify the image target size\n",
    "    img = img.resize((150, 150))\n",
    "    \n",
    "    # Turn the image into a 4D-array\n",
    "    X = np.expand_dims(img, axis =0)\n",
    "    \n",
    "    # Normalize the image\n",
    "    X = X/255.0\n",
    "    \n",
    "    # Turn the image into a Numpy array with float32 data type\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Obtain the image location\n",
    "    url = event['url']\n",
    "    \n",
    "    # Preprocess the image\n",
    "    X = preprocessor(url)\n",
    "    \n",
    "    # Make prediction\n",
    "    preds = predict(X)\n",
    "    \n",
    "    # Obtain the result\n",
    "    results = decode_predictions(preds)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Locally with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created the `lambda_fucntion.py`. Next, we want to take and deploy it using AWS Lambda. For that, we will use Docker. AWS Lambda supports docker, so we can use a container image to deploy our function.\n",
    "\n",
    "In this section, you will learn how to run the model locally using Docker within your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dockerfile`\n",
    "\n",
    "The next step is to create a Dockerfile. __What should we put in the docker file?__\n",
    "\n",
    "- __Dockerfile__ is a way for you to put all the dependencies you need for running the code into one single image that contains everything. \n",
    "\n",
    "\n",
    "- __A Docker image__ is a private file system just for your container. It provides all the files and code your container needs.\n",
    "\n",
    "\n",
    "- This image is self-sufficient because it has everything you need, such as:\n",
    "    - installing the python package management system.\n",
    "    - installing the pillow library to deal with image file.\n",
    "    - installing the TensorFlow Lite tflite_runtime interpreter.\n",
    "    - taking our model in tflite file and copy it to the docker image.\n",
    "    - taking the lambda_function.py and copy it to the docker image.\n",
    "    \n",
    "The file below is the official docker image from Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "```python\n",
    "FROM public.ecr.aws/lambda/python:3.7\n",
    "\n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "RUN pip3 install pillow --no-cache-dir\n",
    "RUN pip3 install https://raw.githubusercontent.com/alexeygrigorev/serverless-deep-learning/master/tflite/tflite_runtime-2.2.0-cp37-cp37m-linux_x86_64.whl --no-cache-dir\n",
    "\n",
    "COPY clothing_classifier.tflite clothing_classifier.tflite\n",
    "COPY lambda_function.py lambda_function.py\n",
    "\n",
    "CMD [ \"lambda_function.lambda_handler\" ]\n",
    "```\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do now is to run and build this docker image, and deploy it using AWS. Another option to deploy the model is by running it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are the steps we do to run the application locally:\n",
    "\n",
    "__Run the docker daemon__. There are 2 ways to do this: \n",
    "- First option is to open __cmd__ as __administrator__, then launch the following command: `\"C:\\Program Files\\Docker\\Docker\\DockerCli.exe\" -SwitchDaemon`\n",
    "    \n",
    "- Second option is to run the __Docker Desktop__ from the start menu and validate that the docker is in __running__ state. \n",
    "    \n",
    "\n",
    "__Build an image from a Dockerfile__. _A Docker image_ is a private file system just for your container. It provides all the files and code your container needs.One important note is that do not change the working directory in Dockerfile\n",
    "\n",
    "```\n",
    "$ docker build -t tf-lite-lambda .\n",
    "```\n",
    "\n",
    "- The command above will build the image from the content of the folder you are currently in, with the tag name `tf-lite-lambda`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Container Image\n",
    "\n",
    "__Start a container based on the image you built in the previous step__. Running a container launches your application with private resources, securely isolated from the rest of your machine.\n",
    "\n",
    "```\n",
    "$ docker run --rm -p 8080:8080 --name clothes-classifier tf-lite-lambda\n",
    "```\n",
    "\n",
    "- The `-p` (stands for _publish_) indicates that we want to map the container port 80 to the host machine port 80. The container opens a Web server on port 80, and we can map ports on our computer to ports exposed by the container.\n",
    "\n",
    "- The `--rm` (stands for _remove_) indicates that we want to automatically remove the cotainer when it exists.\n",
    "\n",
    "- The `--name` gives a name to a new container, and `tf-lite-lambda` is the image name we use to create the container.\n",
    "\n",
    "__Save and share your image on Docker Hub__ to enable other users to easily download and run the image on any destination machine.\n",
    "\n",
    "```\n",
    "$ docker tag tf-lite-lambda [userName]/tf-lite-lambda\n",
    "$ docker push [userName]/tf-lite-lambda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the screenshots of the results from the previous commands:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/build%20n%20run%20the%20docker%20app.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test.py`\n",
    "\n",
    "After we run the model, we want to test it. We need to create a special file that we can call to see the results of what the model has predicted. \n",
    "\n",
    "The file contains:\n",
    "- the complete categories from the expected input image.\n",
    "- a PANTS (test) image obtained from this link: http://bit.ly/mlbookcamp-pants. We will send a request that has a key `url` and a url of the image\n",
    "- a URL address indicating that we deploy on the localhost inside the docker.\n",
    "- a procedure to send a post request to the target URL address to obtain the prediction result.\n",
    "- parsing the prediction result and showing it to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "labels = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'short',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"url\": \"http://bit.ly/mlbookcamp-pants\"\n",
    "}\n",
    "\n",
    "url =\"http://localhost:8080/2015-03-31/functions/function/invocations\"\n",
    "\n",
    "results = requests.post(url, json=data).json()\n",
    "\n",
    "print('[PREDICTION RESULT]')\n",
    "print('+-------------------------------------------+')\n",
    "score = []\n",
    "for cat in results:\n",
    "\tprint('+ {}: {}'.format(cat, results[cat]))\n",
    "\tscore.append(results[cat])\n",
    "\n",
    "best_cat = np.argmax(score)\n",
    "print('+-------------------------------------------+')\n",
    "print('Therefore, the model predicts the input image as {}'.format(labels[best_cat].upper()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `test.py` in your CLI and see the result for yourself:\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/app%20prediction%20result.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just deployed the model locally with Docker. Now, we can bring the same container and deploy it on AWS. AWS has everything you need to deploy your deep learning model online. For this case, we will use AWS CLI, AWS ECR, AWS Lambda, and AWS API Gateway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install AWS CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you do with AWS is an API call. Although you can do by visiting the website, but wouldn't it be nice if you can do it one time? Hence, make sure you have installed AWS CLI in your local machine. https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Your AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to deploy the app on AWS, it's obvious we need to set up an account there. After you make an AWS IAM User account, set up your Access Key ID, Secret Access Key, Default Region, and Default Output Format (commonly JSON). Once we have done this, we can make programmatic calls to AWS from the AWS CLI.\n",
    "\n",
    "```\n",
    "$ aws configure\n",
    "```\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20configure.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Repo in AWS ECR (Elastic Container Registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS ECR is a place for us to put Docker images. By running the following command, we will create a private repository to store the Docker image we have built previously.\n",
    "\n",
    "```\n",
    "$ aws ecr create-repository --repository-name lambda-images\n",
    "```\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20create%20repo.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the Image to the Repo\n",
    "\n",
    "Now, we want to publish the image that we have built locally. The followings are the steps cited directly from AWS (`AWS ECR > Repositories > lambda-images > View Push Command`):\n",
    "\n",
    "- Retrieve an authentication token and authenticate your Docker client to your registry.\n",
    "\n",
    "```\n",
    "$ aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin XXXXXXXXX474.dkr.ecr.us-east-1.amazonaws.com\n",
    "```\n",
    "\n",
    "- Build your Docker image using the following command. \n",
    "\n",
    "```\n",
    "$ docker build -t lambda-images .\n",
    "```\n",
    "\n",
    "- Tag your image so you can push the image to this repository.\n",
    "\n",
    "```\n",
    "$ docker tag tf-lite-lambda XXXXXXXXX474.dkr.ecr.us-east-1.amazonaws.com/lambda-images:tf-lite-lambda\n",
    "```\n",
    "\n",
    "- Run the following command to push this image to your newly created AWS repository.\n",
    "\n",
    "```\n",
    "$ docker push XXXXXXXXX474.dkr.ecr.us-east-1.amazonaws.com/lambda-images:tf-lite-lambda\n",
    "```\n",
    "\n",
    "Check the pushed image on the AWS ECR web page. Make sure to copy the URL because we need it to create a Lambda Function.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20push%20image.jpg' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create a Lambda Function. Go to AWS `Lambda` and click `Create function`. Choose `Container Image`.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20create%20lambda%20func.jpg' width=\"800\">\n",
    "\n",
    "Give your function a unique name and fill in the Container Image URL with the Image URL that you copied earlier. By leaving everything to default, click `Create function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just created a lambda function for a prediction task. However, the current configuration does not give us sufficient memory and timeout. We have a big model and the function will take some time to run and load everything to the memory for the first time. Thus, we need to reconfigure it. Go to `Configuration` > `General Configuration` > click `Edit` and set RAM and Timeout to __512/1024__ and __30__ sec respectively. Save it.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20RAM%20Timeout.jpg' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a test with this JSON file format:\n",
    "```python\n",
    "{\n",
    "    \"url\": \"https://tinyurl.com/clothes-t-shirt\"\n",
    "}\n",
    "```\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20test%20event.jpg' width=\"800\">\n",
    "\n",
    "Give a new event a name and click `Test` after you save it. Then, you will see the following result:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20test%20result.jpg' alt='taken from tfcertification.com' width=\"800\">\n",
    "\n",
    "One thing you need to be aware of is that with AWS Lambda, you will be charged based on the number of requests and the duration, that is, the time it takes for our code to be executed. Please refer to this [link](https://aws.amazon.com/lambda/pricing/) for more pricing info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Gateway Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just tested the function and it seems to work well in making the prediction. What's left is to use it from outside (online). To do this, we need to create an API via AWS API Gateway.\n",
    "\n",
    "__1. Create a New API__\n",
    "\n",
    "- Visit AWS API Gateway, then choose REST API by clicking `Build` button.\n",
    "- Choose the protocol: `REST`. Choose New API for __Creating New API__. Then, fill in the API Name and add some description.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20rest%20api.jpg' width=\"300\"><img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20api%20setting.jpg' width=\"600\">\n",
    "\n",
    "__2. Create a resource: Predict and a method POST__\n",
    "- From `Actions`, choose Make Resource > fill in \"predict\".\n",
    "- From `Actions`, choose Make Method > select `POST`\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20method%20resource.jpg' width=\"200\">\n",
    "\n",
    "__3. Select the Lambda Function and add some details__. \n",
    "- Click on `POST`, then make sure to write the correct name for the __Lambda Function__ and leave everything by default. \n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20predict%20post%20setup.jpg' width=\"500\">\n",
    "\n",
    "__4. Test the API__. \n",
    "- From the flow chart execution, click `Test`.\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20test%20api.jpg' width=\"800\">\n",
    "- To test it, input the following code in the __Request Body__:\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20request%20body.jpg' width=\"400\">\n",
    "- You should see the following result in the __Response Body__:\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20response%20body.jpg' width=\"400\">\n",
    "\n",
    "__5. Deploy the API__\n",
    "- Finally, we need to deploy the API to use it outside. From `Actions`, click `DEPLOY API`. \n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20deploy%20api.jpg' width=\"400\">\n",
    "\n",
    "\n",
    "- Obtain the URL from the \"Invoke URL\" section. In this case, we have: https://xw2bv0y8mb.execute-api.us-east-1.amazonaws.com/test\n",
    "\n",
    "\n",
    "- Open the __Postman App__ or go to [reqbin](https://reqbin.com/) to test the REST API we just created. Notice, since we specify `predict` as our method for `POST`, we need to add `/predict` at the end of the URL. Hence, the complete URL to make an API call for making a prediction is https://xw2bv0y8mb.execute-api.us-east-1.amazonaws.com/test/predict. Copy and paste the link to the URL section in the app.\n",
    "\n",
    "\n",
    "- Copy the following object in JSON as the body to make this POST request. Click `Send`.\n",
    "```javascript\n",
    "{\n",
    "    \"url\": \"https://tinyurl.com/clothes-t-shirt\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "- You can see the prediction result as the content received after making the API call POST request.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20post%20request.jpg'>\n",
    "\n",
    "\n",
    "- Alternatively, we can use `cURL` (stands for client URL) to send the data (in this case, the t-shirt image) in POST request to our service (in this case, the clothes image classifier) via terminal (i.e. Git Bash).\n",
    "```\n",
    "$ curl -d '{\"url\": \"https://tinyurl.com/clothes-t-shirt\"}' -H \"Content-Type: application/json\" -X POST https://xw2bv0y8mb.execute-api.us-east-1.amazonaws.com/test/predict\n",
    "````\n",
    "\n",
    "- Run the command above will generate this prediction result:\n",
    "<img src=\"https://raw.githubusercontent.com/diardanoraihan/E2E_Deep_Learning/main/Visualization/aws%20post%20request%20cmd.jpg\">\n",
    "\n",
    "- Congrats, now your deep learning model is totally online and ready to help the world become a better place!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
